---
title: "Assignment 3"
author: "Group 2"
format: pdf
editor: visual
date: Mar 18 2024
execute: 
  warning: false
  message: false
---

```{r Packages}
library(tidycensus)
library(tidyverse)
library(tidymodels) # collection of packages for machine learning 
library(kknn) # package for k-nearest neighbors models
library(sf)
library(rpart)
library(ranger)
library(vip)

# setting randomization parameter
set.seed(315)
```

```{r Define Data variable name}
vars <- c(
  "B06011_001E", # mean income
  "B19122_001E", # household earner
  "DP03_0025E", # Median Gross Rent (Dollars)
  "B25064_001E", # Mean travel time to work (minutes)
  "B19058_001E",# Public Assistance Income or Food Stamps/SNAP in the Past 12 Months for Households 
  "DP02_0068E",# Total population over 25 years with bachelor's degree or higher
  "DP02_0059E", # Total population over 25 years
  "DP03_0048PE", # Civilian employed population 16 years and over
  "B17001_001E", # total_pop
  "B17001_002E" # below_poverty
  )

ACS <- get_acs(geography = "county", 
                    variables = vars, 
                    year = 2022,
                    survey = "acs1", 
                    output = "wide")

LAT_train <- readxl::read_xlsx("Data Raw/Labor action tracker data 12.4.23.xlsx")
LAT_test <- readxl::read_xlsx("Data Raw/Labor action tracker data 2.26.24.xlsx")
```

## Data Cleaning--LAT
```{r Data Cleaning--LAT Junyi & Edison, output=FALSE}
county <- tigris::counties(cb = TRUE)
#----------
# LAT Train Data
#----------
LAT_train <- LAT_train %>%
  mutate(
    coordinate = ifelse(
      `Number of Locations` > 1,
      strsplit(as.character(`Latitude, Longitude`), ";\\s*"),
      `Latitude, Longitude`
    ))%>%  
  unnest(coordinate)
LAT_train <- separate(LAT_train, coordinate, into = c("lat", "lon"), 
                 sep = ",\\s*", remove = FALSE)
# check is there any NAs 
LAT_train <- LAT_train %>%
  mutate(
    lat = as.numeric(lat),
    lon = as.numeric(lon))
na_summary <- LAT_train %>%
  summarise_all(~ sum(is.na(.)))
# yes, there is one in lon column. 
LAT_train$lon[2830] = -85.73642799999999
# transform LAT data into geometric data frame
LAT_train <- st_as_sf(LAT_train, coords = c("lon", "lat"))
# matching same projection
st_crs(LAT_train) <- st_crs(county)
# joining 2 geometric data frames
LAT_train <- st_join(LAT_train, county)
# housekeeping and make LAT data looks clean and organized
vars <- names(LAT_train)
LAT_train <- LAT_train %>%
  select(c(all_of(vars), NAMELSAD, GEOID)) %>%
  rename(County = NAMELSAD) %>%
  select(1:10, 23, 11:22, 24, 29, 31)

#----------
# LAT Test Data
#----------
LAT_test <- LAT_test %>%
  mutate(
    coordinate = ifelse(
      `Number of Locations` > 1,
      strsplit(as.character(`Latitude, Longitude`), ";\\s*"),
      `Latitude, Longitude`
    ))%>%  
  unnest(coordinate)
LAT_test <- separate(LAT_test, coordinate, into = c("lat", "lon"), 
                 sep = ",\\s*", remove = FALSE)
# check is there any NAs 
LAT_test <- LAT_test %>%
  mutate(
    lat = as.numeric(lat),
    lon = as.numeric(lon))
na_summary <- LAT_test %>%
  summarise_all(~ sum(is.na(.))) # There is no NAs

LAT_test <- st_as_sf(LAT_test, coords = c("lon", "lat"))
# matching same projection
st_crs(LAT_test) <- st_crs(county)
# joining 2 geometric data frames
LAT_test <- st_join(LAT_test, county)
# housekeeping and make LAT data looks clean and organized
vars <- names(LAT_test)
LAT_test <- LAT_test %>%
  select(c(all_of(vars), NAMELSAD, GEOID)) %>%
  rename(County = NAMELSAD) %>%
  select(1:10, 23, 11:22, 24, 29, 31)
```

## Data Cleaning--ACS
```{r Data Cleaning--ACS Junyi & Edison, output=FALSE}
ACS <- ACS %>%  
  select(GEOID, ends_with("E")) %>%
  separate(col = "NAME", into = c("County", "State"), sep = ",") %>%
  rename(
    "median_inc" = "B06011_001E",
    "earners" = "B19122_001E",
    "travel_work"="DP03_0025E",
    "total_pop" = "B17001_001E",
    "below_poverty" = "B17001_002E",
    "rent" = "B25064_001E",
    "assi_income"= "B19058_001E",
    "ba_pop"= "DP02_0068E",
    "total_pop25"="DP02_0059E",
    "employed"="DP03_0048PE" # Civilian employed population 16 years and over
) %>%
  mutate(
    poverty_rate = below_poverty/total_pop,
    ba_rate = ba_pop/total_pop
  )
```

## Training Data and Test Data
```{r Training & Testing Data Junyi & Edison, output=FALSE}
#Here is the final data set with all acs variables and the LAT data
#Since around 90% of the duration was NAs, so I did not unify the time into a new column.

data_train <- left_join(LAT_train, ACS, by="GEOID")
data_train <- as.data.frame(data_train)
# clean out redundant variables
data_train <- data_train  %>%
  select(-ends_with(".x")) %>%
  select(-ends_with(".y"))

data_test <- left_join(LAT_test, ACS, by="GEOID")
data_test <- as.data.frame(data_test)
# clean out redundant variables
data_test <- data_test  %>%
  select(-ends_with(".x")) %>%
  select(-ends_with(".y"))

# A new column I combine the other two labor actions into non-strike 
data_train <- data_train %>% 
  mutate(strike = ifelse(`Strike or Protest`== "Strike",1,0))
data_test <- data_test %>% 
  mutate(strike = ifelse(`Strike or Protest`== "Strike",1,0))

# factor outcome variable
data_test$strike_f <- as.factor(data_test$strike)
data_train$strike_f <- as.factor(data_train$strike)


write_csv(data_train, "Data Clean/data_train.csv")
write_csv(data_test, "Data Clean/data_test.csv")
```

## Logit
```{r LOGSTIC Junyi}
# I also build the log model. This is because I think LASSO implys that the relationship should be linear. However, our dependent variable is a dummy variable. In this case, it would make sense a lot to do logisitic.


#Since run the log, our dependent variable should be a factor, so I create a new variable specifically for the factor version of "strike"


# 1. Specify the Logistic Regression Model
logistic_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

# 2. Prepare the Recipe
logistic_recipe <- 
  recipe(strike_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate+Authorized, data = data_train) |>
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_impute_mean(all_predictors())

# 3. Bundle the Model and Recipe into a Workflow
logistic_workflow <- workflow() %>% 
  add_model(logistic_spec) %>% 
  add_recipe(logistic_recipe)

# 4. Fit the Model to Your Training Data
logistic_fit <- logistic_workflow %>% 
  fit(data = data_train)

# 5. Predict on the Test Data
test_predictions <- logistic_fit %>% 
  predict(new_data = data_test) %>% 
  bind_cols(data_test)


# To convert probabilities to binary outcome (assuming the class of interest is 1)
logistic_test_predictions <- test_predictions %>%
  mutate(predicted_class = .pred_class,actual_class = strike_f)

# 6. Evaluate the Model
logistic_conf_mat <- conf_mat(logistic_test_predictions, truth = actual_class, estimate = predicted_class)

# Print the confusion matrix
print(logistic_conf_mat)

# Optionally, print summary metrics
summary(logistic_conf_mat)

# Visualize the confusion matrix (if desired)
autoplot(logistic_conf_mat)
```


## KNN
```{r KNN Qing}
# Defining the "recipe" to preprocess the data

knn_recipe <- recipe(strike_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate, 
                 data = data_train) |>
  step_scale(all_predictors()) |> # Normalizing standard deviation to one
  step_impute_mean(all_predictors()) # Normalizing mean to one


# KNN Model 1

knn_model1 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 59) # sqrt n


# Setting up the workflow

knn_workflow1 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model1)


# Fitting model to training data

knn_fit1 <- knn_workflow1 |>
  fit(data = data_train)


# Making predictions on the testing data

knn_preds_test <- predict(knn_fit1, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

knn_eval_test <- cbind("pop" = data_test$strike_f, knn_preds_test)


# Evaluating the models with confusion matrices

knn_cm_test1 <- conf_mat(data = knn_eval_test,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
knn_cm_test1

#-------------------------------------------------------------------------------

# KNN Model 2

knn_model2 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 29) # sqrt n/2


# Setting up the workflow

knn_workflow2 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model2)


# Fitting model to training data

knn_fit2 <- knn_workflow2 |>
  fit(data = data_train)


# Making predictions on the testing data

knn_preds_test2 <- predict(knn_fit2, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

knn_eval_test2 <- cbind("pop" = data_test$strike_f, knn_preds_test2)


# Evaluating the models with confusion matrices

knn_cm_test2 <- conf_mat(data = knn_eval_test2,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
knn_cm_test2

#-------------------------------------------------------------------------------

# KNN Model 3

knn_model3 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 119) # sqrt n*2


# Setting up the workflow

knn_workflow3 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model3)


# Fitting model to training data

knn_fit3 <- knn_workflow3 |>
  fit(data = data_train)


# Making predictions on the testing data

knn_preds_test3 <- predict(knn_fit3, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

knn_eval_test3 <- cbind("pop" = data_test$strike_f, knn_preds_test3)


# Evaluating the models with confusion matrices

knn_cm_test3 <- conf_mat(data = knn_eval_test3,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
knn_cm_test3

#-------------------------------------------------------------------------------

# KNN Model 4

knn_model4 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 89) # between model 1 and 3


# Setting up the workflow

knn_workflow4 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model4)


# Fitting model to training data

knn_fit4 <- knn_workflow4 |>
  fit(data = data_train)


# Making predictions on the testing data

knn_preds_test4 <- predict(knn_fit4, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

knn_eval_test4 <- cbind("pop" = data_test$strike_f, knn_preds_test4)


# Evaluating the models with confusion matrices

knn_cm_test4 <- conf_mat(data = knn_eval_test4,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
knn_cm_test4

#-------------------------------------------------------------------------------

# KNN Model 5

knn_model5 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 43) # between model 1 and 2


# Setting up the workflow

knn_workflow5 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model5)


# Fitting model to training data

knn_fit5 <- knn_workflow5 |>
  fit(data = data_train)


# Making predictions on the testing data

knn_preds_test5 <- predict(knn_fit5, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

cnn_eval_test5 <- cbind("pop" = data_test$strike_f, knn_preds_test5)


# Evaluating the models with confusion matrices

knn_cm_test5 <- conf_mat(data = cnn_eval_test5,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
knn_cm_test5

#-------------------------------------------------------------------------------

# 10-fold cross-validation

k <- 10
knn_folds_tm <- vfold_cv(data_test, v = k)


# Fitting the models using `fit_resamples`

knn_fit_cv_tm1 <- knn_workflow1 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm2 <- knn_workflow2 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm3 <- knn_workflow3 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm4 <- knn_workflow4 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm5 <- knn_workflow5 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

# Collecting the mean accuracy rate for each model

knn_success_tm1 <- collect_metrics(knn_fit_cv_tm1)$mean[1]
knn_success_tm2 <- collect_metrics(knn_fit_cv_tm2)$mean[1]
knn_success_tm3 <- collect_metrics(knn_fit_cv_tm3)$mean[1]
knn_success_tm4 <- collect_metrics(knn_fit_cv_tm4)$mean[1]
knn_success_tm5 <- collect_metrics(knn_fit_cv_tm5)$mean[1]


# Displaying the accuracy rates

paste("Model 1 accuracy:", round(knn_success_tm1, 3))
paste("Model 2 accuracy:", round(knn_success_tm2, 3))
paste("Model 3 accuracy:", round(knn_success_tm3, 3))
paste("Model 4 accuracy:", round(knn_success_tm4, 3))
paste("Model 5 accuracy:", round(knn_success_tm5, 3))
```

## LASSO
```{r LASSO Junyi}
# Here I create LASSO through find the best penalty to train my model and create the confusion matrixs

lasso_split <- initial_split(data_train, prop = 0.8, strata = strike_f)
data_lasso_train <- training(lasso_split)
data_lasso_train <- data_lasso_train %>%
  select(-geometry)
data_lasso_test <- testing(lasso_split)
data_lasso_test <- data_lasso_test %>%
  select(-geometry)

lasso_world_recipe <- 
  recipe(strike ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate+Authorized, data = data_lasso_train) |>
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_impute_mean(all_predictors())

lasso_wf <- workflow() |> 
  add_recipe(lasso_world_recipe)

lasso_model <- linear_reg(penalty = .1, mixture = 1) |> 
  set_engine("glmnet")

#Pick the right penalty
lasso_tune_spec <- linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")
# the sequence of penalty parameters to search over
  penalty_grid <- tibble(
  penalty = seq(0, 5, by = .01)
)
# easy way to generate folds for cross validation
lasso_folds <- vfold_cv(data_lasso_train, v = 5)
doParallel::registerDoParallel()


lasso_grid <- tune_grid(
  lasso_wf |> add_model(lasso_tune_spec),
  resamples = lasso_folds,   # 5 fold cross validation
  grid = penalty_grid
)

lasso_grid_metrics <- lasso_grid |>
  collect_metrics() |>
  print()

lasso_grid_metrics |>
  filter(.metric == "rmse") |> 
  ggplot(aes(x = penalty, y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), 
                alpha = 0.5) +
  geom_line(size = 1.5) +
  # scale_x_log10() +
  labs(x = "Penalty", y = "RMSE")


#Use the lowest_rmse to train the model
lowest_rmse <- lasso_grid |>
  select_best("rmse")

final_lasso <- finalize_workflow(
  lasso_wf |> add_model(lasso_tune_spec),
  lowest_rmse
)

final_lasso |>
  fit(data_lasso_train) |>
  extract_fit_parsnip() |>
  vip::vi(lambda = lowest_rmse$penalty) |>
  mutate(
    Importance = abs(Importance),
    Variable = forcats::fct_reorder(Variable, Importance)
  ) |>
  filter(Importance != 0) |> 
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)

last_fit(
  final_lasso,
  lasso_split
) |> 
  collect_metrics()



#Confusion Matrix
final_lasso_fitted <- final_lasso %>% 
  fit(data = data_lasso_train)
test_predictions <- predict(final_lasso_fitted, new_data = data_lasso_test) %>% 
  bind_cols(data_lasso_test)# Binding the predictions with the actual outcomes

lasso_test_predictions <- test_predictions %>%
  mutate(predicted_class = as.factor(if_else(.pred >= 0.5, 1, 0)),
         actual_class = as.factor(strike_f))

# Create a confusion matrix
lasso_conf_mat <- conf_mat(lasso_test_predictions, truth = actual_class, estimate = predicted_class)

# Print the confusion matrix
print(lasso_conf_mat)

# For detailed evaluation metrics
lasso_conf_mat %>%
  summary()
```


## Random Forest
```{r Random Forest Edison}
rf_split <- initial_split(data_train, prop = 0.8, strata = strike)
rf_data_train <- training(rf_split)
rf_data_test <- testing(rf_split)

```

Let's try fitting a *single* random forest using the `rand_forest` model:

```{r}
# Create model specification 
rf_model1 <- rand_forest(
  mtry = 1,   # number of predictors randomly selected at each split
  min_n = 5,  # minimum number of data points in a node for it to be possible to split further
  trees = 50  # no. of trees in the ensemble
  ) %>%  
  set_mode("classification") %>%
  set_engine("ranger",
             importance = "impurity") #allows for variable important calculation 

# Create recipe
rf_recipe1 <- recipe(strike_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed + poverty_rate + ba_rate, data = rf_data_train, na.action = "na.pass")

# Create workflow

rf_wflow1 <- workflow() %>%
  add_recipe(rf_recipe1) %>%
  add_model(rf_model1)

rf_single <- rf_wflow1 |> 
  fit(data=rf_data_train)

vip(rf_single)


```

Random forests have *multiple* hyperparameters than need tuning. 

We will focus on tuning two: `mtry` (the number of predictors considered) and `trees`.  We will use 10-fold cross-validation in the training set to find the appropriate values of the tuning parameters 

Let's create the `rsample` object for using 3 folds. Then, we'll tune the
grid. This time, we'll just specify `grid = 10` to indicate we want 10
hyperparameter configurations. The `dials` package, in this case, will
automatically choose those values for us.

```{r}

rf_model <- rand_forest(
  mtry = tune(),
  min_n = 5,
  trees = tune()) %>%  
  set_mode("classification") %>%
  set_engine("ranger",
             importance = "impurity") 

# Create recipe

rf_recipe <- recipe(union_vote ~ ., data = wern_train)
  
# Create workflow
rf_wflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model)


# Generate cross-validation sets for tuning our model 
trees_folds <- vfold_cv(wern_train, v = 10)

# Create a set of tuning parameter values to search over
rf_grid <- dials::grid_regular(   #automatically constructs a set of values to try
  mtry(range = c(1, ncol(wern_train) - 1)),
  trees(range = c(100,1000)),
  levels = 5
)
```

*STOP.  What is `rf_grid`?  what should it look like it you type `print(rf_grid)` *


```{r}

# Find appropriate tuning parameters
start.time <- Sys.time()
rf_fit <- tune_grid(
  object = rf_wflow, 
  resamples = trees_folds, 
  grid = rf_grid)
end.time <- Sys.time()
end.time - start.time

```

We're going to use a new metric this time called the ROC-AUC. This is the Area Under the Curve for the Receiver-Operator Characteristic curve. The
main takeaway for this metric is that it tells us how good our probability predictions are, rather than just the predictions. In classification settings, it's often preferable to the accuracy. It varies from 0 to 1, where larger is better:

```{r}
# View the ROC metric across models 
rf_fit %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry, trees)
```

Now, let's finalize the workflow:

```{r}
# Select the best RF model 
best_rf_auc <- select_best(rf_fit, "roc_auc")
#best_rf_acc <- select_best(rf_fit, "accuracy")

final_rf_wflow <- rf_wflow %>%
  finalize_workflow(parameters = best_rf_auc) %>%
  fit(data = wern_train)

#visualize the most important features
vip(final_rf_wflow)
```

And evaluate the model's performance:

```{r}
y_hat_oos<-predict(final_rf_wflow, new_data = wern_test) |> 
  as_tibble()

caret::confusionMatrix(data=y_hat_oos$.pred_class, reference=as.factor(wern_test$union_vote))

```

*GROUP EXERCISE: Go back and re-run the "best" RF model, only use accuracy as the metric for the "best" model.  Compare this version of the RF to the one chosen based on ROC-AUC* 
```{r}
# Select the best RF model 
best_acc <- select_best(rf_fit, "accuracy")

final_acc_wflow <- rf_wflow %>%
  finalize_workflow(parameters = best_acc) %>%
  fit(data = wern_train)

#visualize the most important features
vip(final_acc_wflow)
```


```{r}
y_hat_oos<-predict(final_acc_wflow, new_data = wern_test) |> 
  as_tibble()

caret::confusionMatrix(data=y_hat_oos$.pred_class, reference=as.factor(wern_test$union_vote))

```


