---
title: "Assignment 3"
author: "Group 2"
format: pdf
editor: visual
date: Mar 18 2024
execute: 
  warning: false
  message: false
---

```{r Packages}
library(tidycensus)
library(tidyverse)
library(tidymodels) # collection of packages for machine learning 
library(kknn) # package for k-nearest neighbors models
library(sf)

# setting randomization parameter
set.seed(315)
```

```{r Define Data variable name}
vars <- c(
  "B06011_001E", # mean income
  "B19122_001E", # household earner
  "DP03_0025E", # Median Gross Rent (Dollars)
  "B25064_001E", # Mean travel time to work (minutes)
  "B19058_001E",# Public Assistance Income or Food Stamps/SNAP in the Past 12 Months for Households 
  "DP02_0068E",# Total population over 25 years with bachelor's degree or higher
  "DP02_0059E", # Total population over 25 years
  "DP03_0048PE", # Civilian employed population 16 years and over
  "B17001_001E", # total_pop
  "B17001_002E" # below_poverty
  )

ACS <- get_acs(geography = "county", 
                    variables = vars, 
                    year = 2022,
                    survey = "acs1", 
                    output = "wide")

LAT_train <- readxl::read_xlsx("Data Raw/Labor action tracker data 12.4.23.xlsx")
LAT_test <- readxl::read_xlsx("Data Raw/Labor action tracker data 2.26.24.xlsx")
```

## Data Cleaning--LAT
```{r Data Cleaning--LAT Junyi & Edison, output=FALSE}
county <- tigris::counties(cb = TRUE)
#----------
# LAT Train Data
#----------
LAT_train <- LAT_train %>%
  mutate(
    coordinate = ifelse(
      `Number of Locations` > 1,
      strsplit(as.character(`Latitude, Longitude`), ";\\s*"),
      `Latitude, Longitude`
    ))%>%  
  unnest(coordinate)
LAT_train <- separate(LAT_train, coordinate, into = c("lat", "lon"), 
                 sep = ",\\s*", remove = FALSE)
# check is there any NAs 
LAT_train <- LAT_train %>%
  mutate(
    lat = as.numeric(lat),
    lon = as.numeric(lon))
na_summary <- LAT_train %>%
  summarise_all(~ sum(is.na(.)))
# yes, there is one in lon column. 
LAT_train$lon[2830] = -85.73642799999999
# transform LAT data into geometric data frame
LAT_train <- st_as_sf(LAT_train, coords = c("lon", "lat"))
# matching same projection
st_crs(LAT_train) <- st_crs(county)
# joining 2 geometric data frames
LAT_train <- st_join(LAT_train, county)
# housekeeping and make LAT data looks clean and organized
vars <- names(LAT_train)
LAT_train <- LAT_train %>%
  select(c(all_of(vars), NAMELSAD, GEOID)) %>%
  rename(County = NAMELSAD) %>%
  select(1:10, 23, 11:22, 24, 29, 31)

#----------
# LAT Test Data
#----------
LAT_test <- LAT_test %>%
  mutate(
    coordinate = ifelse(
      `Number of Locations` > 1,
      strsplit(as.character(`Latitude, Longitude`), ";\\s*"),
      `Latitude, Longitude`
    ))%>%  
  unnest(coordinate)
LAT_test <- separate(LAT_test, coordinate, into = c("lat", "lon"), 
                 sep = ",\\s*", remove = FALSE)
# check is there any NAs 
LAT_test <- LAT_test %>%
  mutate(
    lat = as.numeric(lat),
    lon = as.numeric(lon))
na_summary <- LAT_test %>%
  summarise_all(~ sum(is.na(.))) # There is no NAs

LAT_test <- st_as_sf(LAT_test, coords = c("lon", "lat"))
# matching same projection
st_crs(LAT_test) <- st_crs(county)
# joining 2 geometric data frames
LAT_test <- st_join(LAT_test, county)
# housekeeping and make LAT data looks clean and organized
vars <- names(LAT_test)
LAT_test <- LAT_test %>%
  select(c(all_of(vars), NAMELSAD, GEOID)) %>%
  rename(County = NAMELSAD) %>%
  select(1:10, 23, 11:22, 24, 29, 31)
```

## Data Cleaning--ACS
```{r Data Cleaning--ACS Junyi & Edison, output=FALSE}
ACS <- ACS %>%  
  select(GEOID, ends_with("E")) %>%
  separate(col = "NAME", into = c("County", "State"), sep = ",") %>%
  rename(
    "median_inc" = "B06011_001E",
    "earners" = "B19122_001E",
    "travel_work"="DP03_0025E",
    "total_pop" = "B17001_001E",
    "below_poverty" = "B17001_002E",
    "rent" = "B25064_001E",
    "assi_income"= "B19058_001E",
    "ba_pop"= "DP02_0068E",
    "total_pop25"="DP02_0059E",
    "employed"="DP03_0048PE" # Civilian employed population 16 years and over
) %>%
  mutate(
    poverty_rate = below_poverty/total_pop,
    ba_rate = ba_pop/total_pop
  )
```

## Training Data and Test Data
```{r Training & Testing Data Junyi & Edison, output=FALSE}
#Here is the final data set with all acs variables and the LAT data
#Since around 90% of the duration was NAs, so I did not unify the time into a new column.

data_train <- left_join(LAT_train, ACS, by="GEOID")
data_train <- as.data.frame(data_train)
# clean out redundant variables
data_train <- data_train  %>%
  select(-ends_with(".x")) %>%
  select(-ends_with(".y"))

data_test <- left_join(LAT_test, ACS, by="GEOID")
data_test <- as.data.frame(data_test)
# clean out redundant variables
data_test <- data_test  %>%
  select(-ends_with(".x")) %>%
  select(-ends_with(".y"))

# A new column I combine the other two labor actions into non-strike 
data_train <- data_train %>% 
  mutate(Strike_or_not = ifelse(`Strike or Protest`== "Strike",1,0))
data_test <- data_test %>% 
  mutate(Strike_or_not = ifelse(`Strike or Protest`== "Strike",1,0))

write_csv(data_train, "Data Clean/data_train.csv")
write_csv(data_test, "Data Clean/data_test.csv")

```

## KNN
```{r KNN Qing}

data_train <- data_train %>% mutate(Strike_or_not_f = as.factor(Strike_or_not))
data_test <- data_test %>% mutate(Strike_or_not_f = as.factor(Strike_or_not))

# Defining the "recipe" to preprocess the data

recipe <- recipe(Strike_or_not_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate, 
                 data = data_train) |>
  step_scale(all_predictors()) |> # Normalizing standard deviation to one
  step_impute_mean(all_predictors()) # Normalizing mean to one


# KNN Model 1

model1 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 59) # sqrt n


# Setting up the workflow

workflow1 <- workflow() |>
  add_recipe(recipe) |>
  add_model(model1)


# Fitting model to training data

fit1 <- workflow1 |>
  fit(data = data_train)


# Making predictions on the testing data

preds_test <- predict(fit1, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

df_eval_test <- cbind("pop" = data_test$Strike_or_not_f, preds_test)


# Evaluating the models with confusion matrices

cm_test1 <- conf_mat(data = df_eval_test,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
cm_test1

#-------------------------------------------------------------------------------

# KNN Model 2

model2 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 29) # sqrt n/2


# Setting up the workflow

workflow2 <- workflow() |>
  add_recipe(recipe) |>
  add_model(model2)


# Fitting model to training data

fit2 <- workflow2 |>
  fit(data = data_train)


# Making predictions on the testing data

preds_test <- predict(fit2, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

df_eval_test <- cbind("pop" = data_test$Strike_or_not_f, preds_test)


# Evaluating the models with confusion matrices

cm_test2 <- conf_mat(data = df_eval_test,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
cm_test2

#-------------------------------------------------------------------------------

# KNN Model 3

model3 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 119) # sqrt n*2


# Setting up the workflow

workflow3 <- workflow() |>
  add_recipe(recipe) |>
  add_model(model3)


# Fitting model to training data

fit3 <- workflow3 |>
  fit(data = data_train)


# Making predictions on the testing data

preds_test <- predict(fit3, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

df_eval_test <- cbind("pop" = data_test$Strike_or_not_f, preds_test)


# Evaluating the models with confusion matrices

cm_test3 <- conf_mat(data = df_eval_test,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
cm_test3

#-------------------------------------------------------------------------------

# KNN Model 4

model4 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 89) # between model 1 and 3


# Setting up the workflow

workflow4 <- workflow() |>
  add_recipe(recipe) |>
  add_model(model4)


# Fitting model to training data

fit4 <- workflow4 |>
  fit(data = data_train)


# Making predictions on the testing data

preds_test <- predict(fit4, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

df_eval_test <- cbind("pop" = data_test$Strike_or_not_f, preds_test)


# Evaluating the models with confusion matrices

cm_test4 <- conf_mat(data = df_eval_test,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
cm_test4

#-------------------------------------------------------------------------------

# KNN Model 5

model5 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 43) # between model 1 and 2


# Setting up the workflow

workflow5 <- workflow() |>
  add_recipe(recipe) |>
  add_model(model5)


# Fitting model to training data

fit5 <- workflow5 |>
  fit(data = data_train)


# Making predictions on the testing data

preds_test <- predict(fit5, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

df_eval_test <- cbind("pop" = data_test$Strike_or_not_f, preds_test)


# Evaluating the models with confusion matrices

cm_test5 <- conf_mat(data = df_eval_test,
                    truth = pop, 
                    estimate = .pred_class)

# Printing confusion matrices

print("Confusion matrix for testing data:")
cm_test5

#-------------------------------------------------------------------------------

# 10-fold cross-validation

k <- 10
folds_tm <- vfold_cv(data_test, v = k)


# Fitting the models using `fit_resamples`

fit_cv_tm1 <- workflow1 |>
  fit_resamples(data = data_train,
                resamples = folds_tm)

fit_cv_tm2 <- workflow2 |>
  fit_resamples(data = data_train,
                resamples = folds_tm)

fit_cv_tm3 <- workflow3 |>
  fit_resamples(data = data_train,
                resamples = folds_tm)

fit_cv_tm4 <- workflow4 |>
  fit_resamples(data = data_train,
                resamples = folds_tm)

fit_cv_tm5 <- workflow5 |>
  fit_resamples(data = data_train,
                resamples = folds_tm)

# Collecting the mean accuracy rate for each model

success_tm1 <- collect_metrics(fit_cv_tm1)$mean[1]
success_tm2 <- collect_metrics(fit_cv_tm2)$mean[1]
success_tm3 <- collect_metrics(fit_cv_tm3)$mean[1]
success_tm4 <- collect_metrics(fit_cv_tm4)$mean[1]
success_tm5 <- collect_metrics(fit_cv_tm5)$mean[1]


# Displaying the accuracy rates

paste("Model 1 accuracy:", round(success_tm1, 3))
paste("Model 2 accuracy:", round(success_tm2, 3))
paste("Model 3 accuracy:", round(success_tm3, 3))
paste("Model 4 accuracy:", round(success_tm4, 3))
paste("Model 5 accuracy:", round(success_tm5, 3))



```

## LASSO
```{r LASSO}


```

## Logit
```{r LOGSTIC Junyi}
# I also build the log model. This is because I think LASSO implys that the relationship should be linear. However, our dependent variable is a dummy variable. In this case, it would make sense a lot to do logisitic.


#Since run the log, our dependent variable should be a factor, so I create a new variable specifically for the factor version of "Strike_or_not"

data_test$factor_strike <- as.factor(data_test$Strike_or_not)
data_train$factor_strike <- as.factor(data_train$Strike_or_not)

# 1. Specify the Logistic Regression Model
logistic_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

# 2. Prepare the Recipe
world_recipe <- 
  recipe(factor_strike ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate+Authorized, data = data_train) |>
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_impute_mean(all_predictors())

# 3. Bundle the Model and Recipe into a Workflow
logistic_workflow <- workflow() %>% 
  add_model(logistic_spec) %>% 
  add_recipe(world_recipe)

# 4. Fit the Model to Your Training Data
logistic_fit <- logistic_workflow %>% 
  fit(data = data_train)

# 5. Predict on the Test Data
test_predictions <- logistic_fit %>% 
  predict(new_data = data_test) %>% 
  bind_cols(data_test)


# To convert probabilities to binary outcome (assuming the class of interest is 1)
test_predictions <- test_predictions %>%
  mutate(predicted_class = .pred_class,actual_class = factor_strike)

# 6. Evaluate the Model
conf_mat <- conf_mat(test_predictions, truth = actual_class, estimate = predicted_class)

# Print the confusion matrix
print(conf_mat)

# Optionally, print summary metrics
summary(conf_mat)

# Visualize the confusion matrix (if desired)
autoplot(conf_mat)


```

## Random Forest
```{r Random Forest Edison}


```
