---
title: "Assignment 3"
author: "Group 2: Guanhao Hu, Qing Yin, Junyi Hua, GianCarlo Samayoa"
format: pdf
editor: visual
date: Mar 18 2024
execute: 
  warning: false
  message: false
---

```{r message = FALSE, error=FALSE, warning=FALSE, results='hide'}
library(tidycensus)
library(tidyverse)
library(tidymodels) # collection of packages for machine learning 
library(kknn) # package for k-nearest neighbors models
library(sf)
library(rpart)
library(ranger)
library(vip)
library(tune)
library(tinytable)

# setting randomization parameter
set.seed(315)
```

```{r message = FALSE, error=FALSE, warning=FALSE, results='hide'}
vars <- c(
  "B06011_001E", # mean income
  "B19122_001E", # household earner
  "DP03_0025E", # Median Gross Rent (Dollars)
  "B25064_001E", # Mean travel time to work (minutes)
  "B19058_001E",# Public Assistance Income or Food Stamps/SNAP in the Past 12 Months for Households 
  "DP02_0068E",# Total population over 25 years with bachelor's degree or higher
  "DP02_0059E", # Total population over 25 years
  "DP03_0048PE", # Civilian employed population 16 years and over
  "B17001_001E", # total_pop
  "B17001_002E" # below_poverty
  )

# census_api_key("8f18e1531b731fb432dd5a9657d8ee937a0290da", install = TRUE, overwrite = TRUE)
ACS <- get_acs(geography = "county", 
                    variables = vars, 
                    year = 2022,
                    survey = "acs1", 
                    output = "wide")

LAT_train <- readxl::read_xlsx("Data Raw/Labor action tracker data 12.4.23.xlsx")
LAT_test <- readxl::read_xlsx("Data Raw/Labor action tracker data 2.26.24.xlsx")
```

```{r message = FALSE, error=FALSE, warning=FALSE, results='hide'}
county <- tigris::counties(cb = TRUE)
#----------
# LAT Train Data
#----------
LAT_train <- LAT_train %>%
  mutate(
    coordinate = ifelse(
      `Number of Locations` > 1,
      strsplit(as.character(`Latitude, Longitude`), ";\\s*"),
      `Latitude, Longitude`
    ))%>%  
  unnest(coordinate)
LAT_train <- separate(LAT_train, coordinate, into = c("lat", "lon"), 
                 sep = ",\\s*", remove = FALSE)
# check is there any NAs 
LAT_train <- LAT_train %>%
  mutate(
    lat = as.numeric(lat),
    lon = as.numeric(lon))
na_summary <- LAT_train %>%
  summarise_all(~ sum(is.na(.)))
# yes, there is one in lon column. 
LAT_train$lon[2830] = -85.73642799999999
# transform LAT data into geometric data frame
LAT_train <- st_as_sf(LAT_train, coords = c("lon", "lat"))
# matching same projection
st_crs(LAT_train) <- st_crs(county)
# joining 2 geometric data frames
LAT_train <- st_join(LAT_train, county)
# housekeeping and make LAT data looks clean and organized
vars <- names(LAT_train)
LAT_train <- LAT_train %>%
  select(c(all_of(vars), NAMELSAD, GEOID)) %>%
  rename(County = NAMELSAD) %>%
  select(1:10, 23, 11:22, 24, 29, 31)

#----------
# LAT Test Data
#----------
LAT_test <- LAT_test %>%
  mutate(
    coordinate = ifelse(
      `Number of Locations` > 1,
      strsplit(as.character(`Latitude, Longitude`), ";\\s*"),
      `Latitude, Longitude`
    ))%>%  
  unnest(coordinate)
LAT_test <- separate(LAT_test, coordinate, into = c("lat", "lon"), 
                 sep = ",\\s*", remove = FALSE)
# check is there any NAs 
LAT_test <- LAT_test %>%
  mutate(
    lat = as.numeric(lat),
    lon = as.numeric(lon))
na_summary <- LAT_test %>%
  summarise_all(~ sum(is.na(.))) # There is no NAs

LAT_test <- st_as_sf(LAT_test, coords = c("lon", "lat"))
# matching same projection
st_crs(LAT_test) <- st_crs(county)
# joining 2 geometric data frames
LAT_test <- st_join(LAT_test, county)
# housekeeping and make LAT data looks clean and organized
vars <- names(LAT_test)
LAT_test <- LAT_test %>%
  select(c(all_of(vars), NAMELSAD, GEOID)) %>%
  rename(County = NAMELSAD) %>%
  select(1:10, 23, 11:22, 24, 29, 31)
```

```{r message = FALSE, error=FALSE, warning=FALSE, results='hide'}
ACS <- ACS %>%  
  select(GEOID, ends_with("E")) %>%
  separate(col = "NAME", into = c("County", "State"), sep = ",") %>%
  rename(
    "median_inc" = "B06011_001E",
    "earners" = "B19122_001E",
    "travel_work"="DP03_0025E",
    "total_pop" = "B17001_001E",
    "below_poverty" = "B17001_002E",
    "rent" = "B25064_001E",
    "assi_income"= "B19058_001E",
    "ba_pop"= "DP02_0068E",
    "total_pop25"="DP02_0059E",
    "employed"="DP03_0048PE" # Civilian employed population 16 years and over
) %>%
  mutate(
    poverty_rate = below_poverty/total_pop,
    ba_rate = ba_pop/total_pop
  )
```

```{r message = FALSE, error=FALSE, warning=FALSE, results='hide'}
#Here is the final data set with all acs variables and the LAT data
#Since around 90% of the duration was NAs, so I did not unify the time into a new column.

data_train <- left_join(LAT_train, ACS, by="GEOID")
data_train <- as.data.frame(data_train)
# clean out redundant variables
data_train <- data_train  %>%
  select(-ends_with(".x")) %>%
  select(-ends_with(".y"))

data_test <- left_join(LAT_test, ACS, by="GEOID")
data_test <- as.data.frame(data_test)
# clean out redundant variables
data_test <- data_test  %>%
  select(-ends_with(".x")) %>%
  select(-ends_with(".y"))

# A new column I combine the other two labor actions into non-strike 
data_train <- data_train %>% 
  mutate(strike = ifelse(`Strike or Protest`== "Strike",1,0))
data_test <- data_test %>% 
  mutate(strike = ifelse(`Strike or Protest`== "Strike",1,0))

# factor outcome variable
data_test$strike_f <- as.factor(data_test$strike)
data_train$strike_f <- as.factor(data_train$strike)

data_train <- data_train %>%
  select(-geometry)
data_test <- data_test %>%
  select(-geometry)


write_csv(data_train, "Data Clean/data_train.csv")
write_csv(data_test, "Data Clean/data_test.csv")
```

```{r message = FALSE, error=FALSE, warning=FALSE}
#, results='hide'}

# I also build the log model. This is because I think LASSO implys that the relationship should be linear. However, our dependent variable is a dummy variable. In this case, it would make sense a lot to do logisitic.


#Since run the log, our dependent variable should be a factor, so I create a new variable specifically for the factor version of "strike"

# 1. Specify the Logistic Regression Model
logistic_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

# 2. Prepare the Recipe
logistic_recipe <- 
  recipe(strike_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate, data = data_train) |>
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_impute_mean(all_predictors())

# 3. Bundle the Model and Recipe into a Workflow
logistic_workflow <- workflow() %>% 
  add_model(logistic_spec) %>% 
  add_recipe(logistic_recipe)

#----------
# Cross-validation
#----------


# create folds
cv_splits_log <- vfold_cv(data_train, v = 10) # 10-fold cross-validation

# Fit the Model to Your Training Data Using Cross-Validation
cv_results_log <- fit_resamples(
  logistic_workflow,
  resamples = cv_splits_log,
  metrics = metric_set(accuracy) # Choose your evaluation metrics
)

#Here I only select the accuracy as the metrics
cv_summary_log <- collect_metrics(cv_results_log)
print(cv_summary_log)



#----------
# Mectrics for LAT Test Data
#----------

logistic_final_fit <- fit(logistic_workflow, data = data_train)


test_predictions <- logistic_final_fit %>% 
  predict(new_data = data_test) %>% 
  bind_cols(data_test)

# To convert probabilities to binary outcome
logistic_new_test_predictions <- test_predictions %>%
  mutate(predicted_class = .pred_class,actual_class = strike_f)

# 6. Evaluate the Model
logistic_test_conf_mat <- conf_mat(logistic_new_test_predictions, truth = actual_class, estimate = predicted_class)
```

```{r}
# Print the confusion matrix
print(logistic_test_conf_mat)
```

```{r}
# Optionally, print summary metrics
summary(logistic_test_conf_mat)
```

```{r results='hide'}
testlog <- summary(logistic_test_conf_mat) %>% slice(1)
testlog <- testlog[,-c(1:2)]
tablog <- testlog %>% tt(digits = 3)
colnames(tablog) <- c("Logistic")
tablog
```

```{r}
# Visualize the confusion matrix (if desired)
autoplot(logistic_test_conf_mat)
```

```{r message = FALSE, error=FALSE, warning=FALSE}
#, results='hide'}


# Defining the "recipe" to preprocess the data

knn_recipe <- recipe(strike_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate, 
                 data = data_train) |>
  step_impute_mean(all_predictors()) |>
  step_scale(all_predictors()) |> # Normalizing standard deviation to one
  step_center(all_predictors())# Normalizing mean to one


# KNN Model 1

knn_model1 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 59) # sqrt n


# Setting up the workflow

knn_workflow1 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model1)


# Fitting model to training data

knn_fit1 <- knn_workflow1 |>
  fit(data = data_train)




#-------------------------------------------------------------------------------

# KNN Model 2

knn_model2 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 29) # sqrt n/2


# Setting up the workflow

knn_workflow2 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model2)


# Fitting model to training data

knn_fit2 <- knn_workflow2 |>
  fit(data = data_train)

#-------------------------------------------------------------------------------

# KNN Model 3

knn_model3 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 119) # sqrt n*2


# Setting up the workflow

knn_workflow3 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model3)


# Fitting model to training data

knn_fit3 <- knn_workflow3 |>
  fit(data = data_train)

#-------------------------------------------------------------------------------

# KNN Model 4

knn_model4 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 89) # between model 1 and 3


# Setting up the workflow

knn_workflow4 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model4)

# Fitting model to training data

knn_fit4 <- knn_workflow4 |>
  fit(data = data_train)

#-------------------------------------------------------------------------------

# KNN Model 5

knn_model5 <- nearest_neighbor(
  mode = "classification", # Our Y is categorical, so we use classification
  engine = "kknn", # Engine for KNN used by tidymodels
  neighbors = 43) # between model 1 and 2


# Setting up the workflow

knn_workflow5 <- workflow() |>
  add_recipe(knn_recipe) |>
  add_model(knn_model5)


# Fitting model to training data

knn_fit5 <- knn_workflow5 |>
  fit(data = data_train)

#-------------------------------------------------------------------------------

# 10-fold cross-validation

knn_folds_tm <- vfold_cv(data_train, v = 10)


# Fitting the models using `fit_resamples`

knn_fit_cv_tm1 <- knn_workflow1 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm2 <- knn_workflow2 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm3 <- knn_workflow3 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm4 <- knn_workflow4 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

knn_fit_cv_tm5 <- knn_workflow5 |>
  fit_resamples(data = data_train,
                resamples = knn_folds_tm)

# Collecting the mean accuracy rate for each model

knn_success_tm1 <- collect_metrics(knn_fit_cv_tm1)$mean[1]
knn_success_tm2 <- collect_metrics(knn_fit_cv_tm2)$mean[1]
knn_success_tm3 <- collect_metrics(knn_fit_cv_tm3)$mean[1]
knn_success_tm4 <- collect_metrics(knn_fit_cv_tm4)$mean[1]
knn_success_tm5 <- collect_metrics(knn_fit_cv_tm5)$mean[1]
```

```{r}
# Displaying the accuracy rates

paste("Model 1 accuracy:", round(knn_success_tm1, 3))
paste("Model 2 accuracy:", round(knn_success_tm2, 3))
paste("Model 3 accuracy:", round(knn_success_tm3, 3))
paste("Model 4 accuracy:", round(knn_success_tm4, 3))
paste("Model 5 accuracy:", round(knn_success_tm5, 3))


# Making predictions on the testing data using Model 5

knn_preds_test5 <- predict(knn_fit5, 
                      new_data = data_test, 
                      type = "class") # Classification

# Binding the actual and predicted Y's together in data frames

knn_eval_test5 <- cbind("pop" = data_test$strike_f, knn_preds_test5)


# Evaluating the models with confusion matrices

knn_cm_test5 <- conf_mat(data = knn_eval_test5,
                    truth = pop, 
                    estimate = .pred_class)
```

```{r}
# Printing confusion matrices
print("Confusion matrix for testing data:")
knn_cm_test5
```

```{r}
#what is the accuracy here based on the testing data?
summary(knn_cm_test5)

```

```{r results='hide'}
testknn <- summary(knn_cm_test5) %>% slice(1)
testknn <- testknn[,-c(1:2)]
testknn$Accuracy <- testknn$.estimate
tabknn <- testknn %>% tt(digits = 3)
colnames(tabknn) <- c("KNN")
tabknn
```

## LASSO

```{r message = FALSE, error=FALSE, warning=FALSE}
#, results='hide'}

# Here I create LASSO through find the best penalty to train my model and create the confusion matrixs
lasso_world_recipe <- 
  recipe(strike ~ median_inc + earners + travel_work + rent +
          assi_income + employed+poverty_rate+ba_rate, data = data_train) |>
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_impute_mean(all_predictors())

lasso_wf <- workflow() |> 
  add_recipe(lasso_world_recipe)

lasso_model <- linear_reg(penalty = .1, mixture = 1) |> 
  set_engine("glmnet")

#Pick the right penalty
lasso_tune_spec <- linear_reg(penalty = tune(), mixture = 1) |> 
  set_engine("glmnet")
# the sequence of penalty parameters to search over
penalty_grid <- tibble(
  penalty = seq(0, 0.005, by = .00001)
)

#----------
# Cross-validation for accuracy
#----------

# easy way to generate folds for cross validation
lasso_folds <- vfold_cv(data_train, v = 10)
doParallel::registerDoParallel()


lasso_grid <- tune_grid(
  lasso_wf |> add_model(lasso_tune_spec),
  resamples = lasso_folds,   # 10 fold cross validation
  grid = penalty_grid
)

#lasso_grid_metrics <- lasso_grid |>
#  collect_metrics() |>
#  print()


lasso_results <- lasso_grid %>%
  collect_metrics() 

best_accuracy <- lasso_results %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean)) %>%
  slice(1) %>%
  pull(penalty)

final_lasso <- finalize_workflow(
  lasso_wf |> add_model(lasso_model),
  list(penalty = best_accuracy)
)

print(best_accuracy)

#----------
# Cross-validation for lowest rmse
#----------

lasso_grid_metrics <- lasso_grid |>
  collect_metrics() |>
  print()

lasso_grid_metrics |>
  filter(.metric == "rmse") |> 
  ggplot(aes(x = penalty, y = mean)) +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), 
                alpha = 0.5) +
  geom_line(size = 1.5) +
  # scale_x_log10() +
  labs(x = "Penalty", y = "rmse")


#Use the lowest_rmse to train the model
lowest_rmse <- lasso_grid |>
  select_best("rmse")

final_lasso <- finalize_workflow(
  lasso_wf |> add_model(lasso_tune_spec),
  lowest_rmse
)

final_lasso |>
  fit(data_train) |>
  extract_fit_parsnip() |>
  vip::vi(lambda = lowest_rmse$penalty) |>
  mutate(
    Importance = abs(Importance),
    Variable = forcats::fct_reorder(Variable, Importance)
  ) |>
  filter(Importance != 0) |> 
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)


#----------
# Matrixs for LAT Test Data for the lowest rmse
#----------

final_lasso_fitted_test <- final_lasso %>% 
  fit(data = data_train)

test_predictions <- predict(final_lasso_fitted_test, new_data = data_test) %>% 
  bind_cols(data_test)# Binding the predictions with the actual outcomes

lasso_new_test_predictions <- test_predictions %>%
  mutate(predicted_class = as.factor(if_else(.pred >= 0.5, 1, 0)),
         actual_class = strike_f)

# Create a confusion matrix
lasso_new_conf_mat <- conf_mat(lasso_new_test_predictions, truth = actual_class, estimate = predicted_class)

# Print the confusion matrix
print(lasso_new_conf_mat)
```

```{r}
# For detailed evaluation metrics
lasso_new_conf_mat %>%
  summary()
```

```{r results='hide'}
testlasso <- lasso_new_conf_mat %>%
                  summary() %>% slice(1)
testlasso <- testlasso[,-c(1:2)]
tablasso <- testlasso %>% tt(digits = 3)
colnames(tablasso) <- c("Lasso")
tablasso
```

```{r message = FALSE, error=FALSE, warning=FALSE}
#, results='hide'}

# Create recipe
rf_recipe1 <- recipe(strike_f ~ median_inc + earners + travel_work + rent +
          assi_income + employed + poverty_rate + ba_rate, data = data_train) %>%
  step_impute_mean(all_predictors())

rf_model <- rand_forest(
  mtry = tune(),
  min_n = 5,
  trees = tune()) %>%  
  set_mode("classification") %>%
  set_engine("ranger",
             importance = "impurity") 

# Create workflow
rf_wflow <- workflow() %>%
  add_recipe(rf_recipe1) %>%
  add_model(rf_model)


# Generate cross-validation sets for tuning our model 
rf_folds <- vfold_cv(data_train, v = 10)

# Create a set of tuning parameter values to search over
rf_grid <- dials::grid_regular(
  mtry(range = c(1, 8)),
  trees(range = c(10, 1000)),
  levels = 5
)

rf_model2 <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = tune()) %>%  
  set_mode("classification") %>%
  set_engine("ranger",
             importance = "impurity") 

# Create workflow
rf_wflow2 <- workflow() %>%
  add_recipe(rf_recipe1) %>%
  add_model(rf_model2)

# Create a set of tuning parameter values to search over
rf_grid2 <- dials::grid_regular(
  mtry(range = c(1, 8)),
  trees(range = c(10, 1000)),
  min_n(range = c(5, 10)),
  levels = 5
)


# Find appropriate tuning parameters
rf_fit <- tune_grid(
  object = rf_wflow, 
  resamples = rf_folds, 
  grid = rf_grid)

rf_fit2 <- tune_grid(
  object = rf_wflow2, 
  resamples = rf_folds, 
  grid = rf_grid2)

# View the accuracy metric across models 
rf_fit %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, mtry, trees)

rf_fit2 %>% 
  collect_metrics() %>%
  filter(.metric == "accuracy")
print(rf_fit2)

# Select the best RF model 
rf_best_auc <- select_best(rf_fit, "accuracy")
rf_best_auc2 <- select_best(rf_fit2, "accuracy")

rf_wflow3 <- rf_wflow %>%
  finalize_workflow(parameters = rf_best_auc) %>%
  fit(data = data_train)

rf_wflow4 <- rf_wflow2 %>%
  finalize_workflow(parameters = rf_best_auc2) %>%
  fit(data = data_train)

vip(rf_wflow3)
vip(rf_wflow4)

rf_OOB_1 <- predict(rf_wflow3, new_data = data_test) %>%
  as_tibble()
rf_cm_1 <- caret::confusionMatrix(data=rf_OOB_1$.pred_class, reference = data_test$strike_f)

rf_OOB_2 <- predict(rf_wflow4, new_data = data_test) %>% 
  as_tibble()
rf_cm_2 <- caret::confusionMatrix(data=rf_OOB_2$.pred_class, reference = data_test$strike_f)

print(rf_cm_1)
```

```{r}
print(rf_cm_2)
```

```{r results = 'hide'}
#making table of Balanced Accuracy - in the case of RF its accuracy

tabrf <- rf_cm_2[["overall"]][["Accuracy"]] %>% as.data.frame() %>% tt(digits = 3)
colnames(tabrf) <- c("RF")
tabrf
```

## Introduction

In this project our group built and compared four predictive models (Logistic Regression, KNN, LASSO, and Random Forest) in order to most accurately predict whether labor action in the Labor Action Tracker (LAT) data set is a “strike” \[1\] or “non-strike” \[0\]. All models were considered on the basis of accuracy and not area under the ROC curve because of the priority placed on correct predictions over few false positives/negatives and because of the balanced predictor variable classes.

Our logistical regression provides $55.4\%$ accuracy in predicting labor action for unseen data. With regard to the LASSO model that has the lowest root-mean-square error (RMSE) when setting the penalty level at 0, the accuracy of its prediction is $55.8\%$. After tuning number of trees, number of randomly sampled predictors and minimum number of data points in a node, the Random Forest model reports $53.8\%$ accuracy in predicting with a best performing parameter–4 randomly sampled predictors, 10 decision trees and minimum 8 data points in each node.

The K-nearest neighbors (KNN) model number 5 selecting 43 neighbors was chosen as the final model to predict labor actions. The goal of this project is to predict labor actions as accurately as possible. Seeing that the KNN model was the most accurate of all the models tested, at a $58\%$ accuracy rate, it was chosen over all other models. *(see table 1)*

```{r}
tottab <- cross_join(tablog, tabknn)
tottab <- cross_join(tottab, tablasso) 
tottab <- cross_join(tottab, tabrf)
print("Table 1: Accuracy of Each Model (% of correct predictions)")
tottab
```

Furthermore, as mentioned earlier, this decision was made because when predicting labor actions, it's crucial to prioritize accuracy above all other evaluation metrics. To explain, the cost of failing to predict most labor actions correctly is likely higher than the cost of having more false positives or false negatives. In other words, the KNN model that selects 43 neighbors was chosen because it had the highest recorded accuracy in predicting labor actions based on the testing data set, making it the most cost-minimizing option for any real-world applications.

## Data Manipulation

In cleaning and preparing the LAT dataset there were four major changes made: Data Transformation into Tidy Format, Parsing Coordinate Variable, Merging New Variables into the Data Set, and Creation of New Variables

1.  Each row was manipulated so that it represented one observation instead of multiple. This was done by duplicating rows based on whether the values in the “number of locations” column was greater than one.

2.  The “Latitude, Longitude” column was split into separate respective variables (“lat”, “lon”).

3.  The “GEOID” column in the countries data set from the tigris library package was used as a key to merge the LAT data set with an imported ACS county-level dataset. In general, our group intends to examine worker’s financial situations, costs to work and basic demographic features as strong indicators in predicting a possible strike, and thus, from the ACS county data set ten variables were imported and merged into the LAT dataset:

    -   Median income, total household earners, public assistance for past 12 months, total population below poverty population, mean travel time to work, median gross rent, total population over 25, total population over 25 with BA degree, population over 16 years employed.  

4.  From the new variables merged into the LAT data set, variables for poverty rate and college degree rate were made. Poverty rate was created by dividing the total population below poverty by the total population, and the college (BA) degree rate was created by dividing the college degree population over 25 by the total population.

With regards to missing data, there were only two instances in which it was encountered. The first was one longitude data missing for action in “2023-06-30 13:47:25” from the LAT training data set during the data manipulating process. This was resolved by replacing it with the corresponding longitude value from the LAT training data set. Second, when creating models for prediction, all predictor variable missing values were replaced with their corresponding means and then normalized.

## Model Assessment

This part will show the detailed steps of how the model has been trained and how well the final model performs in the validation sets and test data.

First, it is important to note that not all of the variables present in the “new” LAT data set were used, only the following variables were used: median income, household earners, mean travel time to work, median rent, public assistance for past 12 months, total employed population, poverty rate, and college degree rate. All other variables were excluded due to being used to create the two new variables college degree rate and poverty rate, and if included could bias the results.

The prediction model had the predicted variable \`strike_f\` (categorical variable for strike or non-strike as \[1\] or \[0\] ) on the left hand side and the aforementioned eight variables on the right hand side. All predictor variables are normalized. Then, five different KNN models are made using the following values for k (nearest neighbors): square root of $n=59$, square root of $n/2=29$, square root of $2*n=119$, halfway between the the square root of n and square root of $2*n=89$, and halfway between square root of n and square root of $n/2=43$, with n as the number of observations. Choosing such diverse *k* values, and points in between, for KNN model development can help identify the optimal balance between sensitivity to local data structures and generalization ability, aiming to enhance model accuracy by exploring a range of model complexities. This approach can help find a *k* that minimizes bias and variance, which is critical for achieving the best predictive performance.

Then, workflows were set up and 5 KNN models (with unique *k* values) were fitted to LAT training data. 10-fold cross validation was used to resample each of the 5 KNN models, and model number 5 ($k=43$) had the highest mean accuracy rate. 

KNN model number 5 in the validation data set performed almost equally as well as it did with the test set, having an accuracy rate of $57.9\%$ and $58.01\%$ respectively. The little change in accuracy from the cross validation to the test data set indicates a high probability of it being unlikely that the current model is overfit to the training data set, strengthening the validity of the models prediction capabilities. Additionally, it is important to note that KNN models do not offer any indication of which predictor variables were most important in determining the prediction accuracy. 

```{r results='hide'}
library(ggplot2)

knn_cm_test5_accuracy <- summary(knn_cm_test5) %>% slice(1)
# Assuming knn_cm_test5_accuracy is a tibble or data frame
knn_cm_test5_accuracy_value <- knn_cm_test5_accuracy %>% 
  pull(.estimate)

# Step 1: Create a data frame of accuracy rates
model_names <- c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5","Model 5 in Test Data")
accuracy_rates <- c(knn_success_tm1, knn_success_tm2, knn_success_tm3, knn_success_tm4, knn_success_tm5, knn_cm_test5_accuracy_value)

# Round the accuracy rates to two decimal places before creating the plot
accuracy_rates <- round(accuracy_rates, 4) # Here, rounding to 4 decimal places for more precision

# Then create your accuracy_data data frame
accuracy_data <- data.frame(Model = model_names, Accuracy = accuracy_rates)

# Now plot the data with accuracy rates labeled on the bars
ggplot(accuracy_data, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  geom_text(aes(label = sprintf("%.2f%%", Accuracy * 100)),
            position = position_dodge(width = 0.9),
            vjust = -0.2,
            color = "black") +
  theme_minimal() +
  labs(title = "KNN Models Accuracy Rates in Validation Set and Test Data", 
       x = "Model", y = "Accuracy Rate") +
  scale_fill_brewer(palette = "Pastel1")
```

## Conclusions

One of the major weaknesses of KNN model number 5 is that every time the model is run, despite setting a seed to keep random variation consistent, the accuracy of the model changes by one or two percentage points. However, of the five models, KNN model number 5 consistently remains the most accurate even after cross-validation. Another major weakness of the model is that the optimum number of nearest neighbors has not been identified; this can be a point of improvement. 

Based on this analysis it cannot be said that KNN model number 5 is the best prediction model for predicting labor actions, due to not knowing the best number(s) of nearest neighbors and its sensitivity to changes in it. Additionally, this model is likely not suited for long term use due to more information being added over time, due to its sensitivity to noisy data.

​​The usefulness of KNN model number 5 is somewhat limited by its variability and the lack of optimization for the number of nearest neighbors. While it may not be the most reliable for long-term predictions, it seems to have some immediate applicability for short-term predictions about whether a labor action would be a strike. This could be useful for organizations to prepare for immediate labor disputes, although the implications must be carefully considered, given the model's sensitivity to data changes. 

In terms of policy implications, the model's current state suggests caution should be taken when using it as the basis for decision-making. If the model is to be used in policy decisions, it would be important to:

1.  Establish clear protocols for model updates, including regular recalibration with new data.

2.  Identify the optimal number of neighbors to improve prediction stability.

3.  Consider combining the KNN model with other models or approaches to mitigate its weaknesses and enhance overall predictive accuracy.

Policymakers might use this model to prepare for imminent labor actions, but they should be aware of its limitations and ensure that decisions are supported by multiple data points and analyses, not solely on the model's predictions. Additionally, they should stay vigilant to the evolving nature of the data and be ready to adapt the model as needed to maintain its relevance and accuracy.
